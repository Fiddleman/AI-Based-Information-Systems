{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pmaSgKNypWv6"],"mount_file_id":"1N9qQVVNkAhLIGQ6pIug7oK6jMlxNKWiP","authorship_tag":"ABX9TyOITgxonMx8DoB/vIz/KYCg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data and Feature Engineering with Python"],"metadata":{"id":"wFJDAOHiKmdy"}},{"cell_type":"markdown","source":["This notebook will cover the major concepts presented in the lecture \"Data and Feature Engineering with Python\" by Dr. Dominik Jung. We will dive into different areas of data engineering, including data preprocessing, feature engineering, exploratory data analysis (EDA), and practical use of popular Python libraries such as Pandas, Scikit-learn, and Matplotlib. This notebook will also include exercises, examples, and insights to help you develop a deep understanding of the data engineering process."],"metadata":{"id":"fwtct0V9pT2j"}},{"cell_type":"markdown","source":["### Prerequisites\n"],"metadata":{"id":"pmaSgKNypWv6"}},{"cell_type":"markdown","source":["Let's start with setting up our libraries"],"metadata":{"id":"BNKCvnh5pbSL"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, SimpleImputer\n","from sklearn.decomposition import PCA\n","from sklearn.impute import SimpleImputer"],"metadata":{"id":"7UaxIiOtpXVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then connect our Google Drive with Google Colab *(run only if you using Google Colab)*"],"metadata":{"id":"-lbKdgSbMCuS"}},{"cell_type":"code","source":["# Acess your files from Google Drive in Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLFtrNS8MHSx","executionInfo":{"status":"ok","timestamp":1729258968232,"user_tz":-120,"elapsed":14979,"user":{"displayName":"Dominik Jung","userId":"17886544687226374865"}},"outputId":"0e23e2da-e923-4ec9-c0ae-b6c3afa36abb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["### 1. Data Imports\n","Data imports are a foundational part of data engineering. Below, we explore how to import data from different sources and ensure it is in a usable form.\n","\n","- **File Formats**: CSV, JSON, SQL Databases, etc."],"metadata":{"id":"aM1ku2x7p2hM"}},{"cell_type":"code","source":["# Example of reading a CSV file\n","data = pd.read_csv('data.csv')\n","print(data.head())"],"metadata":{"id":"5BeDc1arp-T2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Handling Different File Types**: We often encounter data in delimited text files, JSON, databases, etc.\n"],"metadata":{"id":"ZdTlB2i8qLsS"}},{"cell_type":"code","source":["# Read JSON\n","json_data = pd.read_json('data.json')\n","\n","# Using SQL Databases\n","import sqlite3\n","conn = sqlite3.connect('database.db')\n","sql_data = pd.read_sql_query(\"SELECT * FROM table_name\", conn)"],"metadata":{"id":"hO8CLiAzqNE6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Exploratory Data Analysis\n","EDA helps understand the data, identify patterns, detect anomalies, and more.\n","\n","**Please update the data variable based on your imports**\n","\n","- **Descriptive Statistics**\n"],"metadata":{"id":"Dr-e7E5UqOl2"}},{"cell_type":"code","source":["# Generate summary statistics\n","data.describe()"],"metadata":{"id":"Hvz3v8UsqRQf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Visualization**: Matplotlib is often used to visualize data.\n"],"metadata":{"id":"TwM5DMGrqW1z"}},{"cell_type":"code","source":["# Histogram\n","plt.hist(data['column_name'])\n","plt.xlabel('Values')\n","plt.ylabel('Frequency')\n","plt.title('Histogram of column_name')\n","plt.show()"],"metadata":{"id":"LmQwZgGJqX-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Correlation and Scatterplots**\n"],"metadata":{"id":"_lHVllV0qlj0"}},{"cell_type":"code","source":["# Correlation matrix\n","corr_matrix = data.corr()\n","plt.matshow(corr_matrix, cmap='coolwarm')\n","plt.colorbar()\n","plt.show()\n","\n","# Scatter plot to visualize relationships\n","plt.scatter(data['feature1'], data['feature2'])\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Scatter Plot of Feature 1 vs Feature 2')\n","plt.show()\n"],"metadata":{"id":"AhxmSMl8qnk-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Data Preprocessing\n","Data preprocessing is crucial for AI systems, as it can significantly affect model performance. Key steps include handling missing values, encoding categorical variables, and scaling.\n","\n","- **Handling Missing Data**\n"],"metadata":{"id":"n-gSdefYqo8j"}},{"cell_type":"code","source":["# Filling missing values using SimpleImputer\n","imputer = SimpleImputer(strategy='mean')\n","data_imputed = imputer.fit_transform(data)"],"metadata":{"id":"5vqXVXDmqsHa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Encoding Categorical Variables**\n"],"metadata":{"id":"bLZrTvC9qvHn"}},{"cell_type":"code","source":["# One-hot encoding\n","encoder = OneHotEncoder()\n","categorical_encoded = encoder.fit_transform(data[['categorical_feature']])\n"],"metadata":{"id":"XMjabaAqqx-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Standardization**\n"],"metadata":{"id":"POfNLUgTqyyl"}},{"cell_type":"code","source":["scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(data)"],"metadata":{"id":"y6hzwq8Fqzju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Feature Engineering\n","Feature engineering involves creating new features, transforming data, and selecting relevant features to improve model performance.\n","\n","- **Feature Creation**: Creating new features from existing ones. For example, extracting day, month, or year from a datetime feature.\n"],"metadata":{"id":"CMSeVlFQq2qJ"}},{"cell_type":"code","source":["# Example: Creating a new feature\n","import datetime\n","data['year'] = pd.DatetimeIndex(data['date']).year\n","data['month'] = pd.DatetimeIndex(data['date']).month\n"],"metadata":{"id":"4vIdgldGq3CS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **The Curse of Dimensionality**: Feature selection can help mitigate the problems of high-dimensional datasets.\n"],"metadata":{"id":"89pwsRo7q5xM"}},{"cell_type":"code","source":["# Example of feature selection using Variance Threshold\n","from sklearn.feature_selection import VarianceThreshold\n","selector = VarianceThreshold(threshold=0.1)\n","selected_features = selector.fit_transform(data)"],"metadata":{"id":"qjEMg-tmq684"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Dimensionality Reduction\n","Dimensionality reduction is essential to avoid overfitting, reduce computational costs, and improve interpretability.\n","\n","- **PCA (Principal Component Analysis)**: PCA helps reduce dimensions by projecting data onto the principal components."],"metadata":{"id":"Wzqwq0T9q9ZL"}},{"cell_type":"code","source":["pca = PCA(n_components=2)\n","principal_components = pca.fit_transform(scaled_data)\n","plt.scatter(principal_components[:, 0], principal_components[:, 1])\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.title('PCA Projection')\n","plt.show()\n"],"metadata":{"id":"27i7zXPGrB-2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Exercises\n","- **Exercise 1**: Load the provided dataset and perform missing value imputation using median strategy.\n","- **Exercise 2**: Conduct exploratory data analysis on a dataset of your choice and visualize at least three different types of plots.\n","- **Exercise 3**: Apply one-hot encoding to a categorical variable and use PCA to reduce dimensionality.\n","\n","### Extra Challenge:\n","Implement a function for target-based encoding and compare the performance of models using binary, target-based, and one-hot encoding.\n"],"metadata":{"id":"rxqVhP-2rI_3"}},{"cell_type":"code","source":["# Example function for percentile calculation\n","\n","def percentile(data, p):\n","    sorted_data = sorted(data)\n","    index = int(p / 100.0 * len(sorted_data))\n","    return sorted_data[index]\n","\n","# Testing the function\n","sample_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","print(\"90th percentile:\", percentile(sample_data, 90))\n"],"metadata":{"id":"7rNVpKW_rMBD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Resources for Further Learning\n","- Official Python documentation: https://docs.python.org/3/\n","- Hands-on Machine Learning with Scikit-learn, Keras, and TensorFlow by Aurélien Géron\n"],"metadata":{"id":"2_VWwcMMpmI9"}},{"cell_type":"markdown","source":[],"metadata":{"id":"oN7rVd_ErIRE"}}]}